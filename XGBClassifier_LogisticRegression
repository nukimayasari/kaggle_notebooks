{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8671396,"sourceType":"datasetVersion","datasetId":5196907}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nukimayasari/xgb-classifier-and-logistic-regression?scriptVersionId=194727087\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-30T18:39:40.106415Z","iopub.execute_input":"2024-08-30T18:39:40.106799Z","iopub.status.idle":"2024-08-30T18:39:42.960325Z","shell.execute_reply.started":"2024-08-30T18:39:40.106767Z","shell.execute_reply":"2024-08-30T18:39:42.958947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **XGB Classifier and Logistic Regression**\n\nIn this notebook, we explore two powerful classification techniques: XGB Classifier and Logistic Regression. Both methods are widely used in machine learning for solving classification problems, but they approach the task in different ways. Our goal is to understand these models, compare their performance, and see how they fare on our dataset.","metadata":{}},{"cell_type":"markdown","source":"# **Introduction**\n\n* **Logistic Regression**\nLogistic Regression is a classic algorithm used for binary classification problems. Despite its name, it is a classification algorithm that predicts the probability of a binary outcome based on one or more predictor variables. It uses the logistic function to model the probability of the default class, transforming raw prediction scores into probabilities between 0 and 1.\n\n* **XGB Classifier**\nXGB Classifier, short for eXtreme Gradient Boosting, is a more advanced ensemble technique based on gradient boosting. It builds a series of decision trees, where each tree corrects the errors made by the previous ones. XGBoost is known for its efficiency, scalability, and high performance in various machine learning tasks.","metadata":{}},{"cell_type":"markdown","source":"# **Dataset**\n\nThe dataset used in this analysis contains information about various factors that could influence heart disease. The columns in the dataset are:\n\n* **Age**: Age of the individual (years).\n* **Gender**: Gender of the individual (Male/Female).\n* **Cholesterol**: Cholesterol level in mg/dL.\n* **Blood Pressure**: Systolic blood pressure in mmHg.\n* **Heart Rate**: Heart rate in beats per minute.\n* **Smoking**: Smoking status (Never/Former/Current).\n* **Alcohol Intake**: Alcohol intake frequency (None/Moderate/Heavy).\n* **Exercise Hours**: Hours of exercise per week.\n* **Family History**: Family history of heart disease (Yes/No).\n* **Diabetes**: Diabetes status (Yes/No).\n* **Obesity**: Obesity status (Yes/No).\n* **Stress Level**: Stress level on a scale of 1 to 10.\n* **Blood Sugar**: Fasting blood sugar level in mg/dL.\n* **Exercise Induced Angina**: Presence of exercise-induced angina (Yes/No).\n* **Chest Pain Type**: Type of chest pain experienced (Typical Angina/Atypical Angina/Non-anginal Pain/Asymptomatic).\n* **Heart Disease**: Target variable indicating presence of heart disease (0: No, 1: Yes).","metadata":{}},{"cell_type":"code","source":"#Loading our data\n\nheart_data = pd.read_csv(\"/kaggle/input/heart-disease-prediction/heart_disease_dataset.csv\", index_col=0)\nheart_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-30T19:12:39.152883Z","iopub.execute_input":"2024-08-30T19:12:39.1533Z","iopub.status.idle":"2024-08-30T19:12:39.188325Z","shell.execute_reply.started":"2024-08-30T19:12:39.153265Z","shell.execute_reply":"2024-08-30T19:12:39.186992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing: Summary Statistics**\n\nBefore diving into model training, it's crucial to understand the basic statistical properties of the dataset. The following summary statistics provide insights into the distribution and range of numerical features related to heart disease:","metadata":{}},{"cell_type":"code","source":"heart_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:43.026416Z","iopub.execute_input":"2024-08-30T18:39:43.026771Z","iopub.status.idle":"2024-08-30T18:39:43.066177Z","shell.execute_reply.started":"2024-08-30T18:39:43.02674Z","shell.execute_reply":"2024-08-30T18:39:43.064688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Interpretation**\n\n* **Cholesterol**: The cholesterol levels range from a minimum of 150 mg/dL to a maximum of 349 mg/dL, with a mean of 249.94 mg/dL. The high standard deviation (57.91) indicates considerable variation in cholesterol levels among individuals.\n\n* **Blood Pressure**: Systolic blood pressure ranges from 90 mmHg to 179 mmHg. The mean value is 135.28 mmHg, with a standard deviation of 26.39, suggesting a broad range of blood pressure readings in the dataset.\n\n* **Heart Rate**: The heart rate varies between 60 and 99 beats per minute, with a mean of 79.20 bpm. The standard deviation is relatively low (11.49), indicating that heart rate values are more clustered around the mean.\n\n* **Exercise Hours**: The number of exercise hours per week ranges from 0 to 9 hours, with a mean of 4.53 hours. The relatively high standard deviation (2.93) shows variability in exercise habits among individuals.\n\n* **Stress Level**: Stress levels, on a scale from 1 to 10, have a mean of 5.65 and range from 1 to 10. The standard deviation (2.83) reflects some variability in perceived stress among the dataset's individuals.\n\n* **Blood Sugar**: Fasting blood sugar levels range from 70 mg/dL to 199 mg/dL. The mean blood sugar level is 134.94 mg/dL, with a standard deviation of 36.70, suggesting a wide range of values.\n\n* **Heart Disease**: The target variable indicating the presence of heart disease shows that 39.2% of individuals in the dataset have heart disease (1), while 60.8% do not (0). The mean of 0.39 and the standard deviation of 0.49 highlight the imbalance in class distribution.","metadata":{}},{"cell_type":"markdown","source":"# **Missing Data Analysis**\nBefore proceeding with data analysis and model building, it's essential to check for missing values in the dataset, as they can impact the accuracy and reliability of the predictive models. The following code was used to identify the number of missing values in each column of the dataset:","metadata":{}},{"cell_type":"code","source":"heart_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:43.068899Z","iopub.execute_input":"2024-08-30T18:39:43.069241Z","iopub.status.idle":"2024-08-30T18:39:43.078678Z","shell.execute_reply.started":"2024-08-30T18:39:43.069211Z","shell.execute_reply":"2024-08-30T18:39:43.077445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:**\n\nThe Alcohol Intake column has 340 missing values, representing a significant portion of the dataset. Handling this missing data will be crucial, whether through imputation, removal, or other methods, to ensure the dataset remains robust for modeling.\n\nAll other features have no missing values, meaning they are complete and ready for further analysis.","metadata":{}},{"cell_type":"markdown","source":"# **Exploratory Data Analysis (EDA) on Risk Factors for Heart Disease**","metadata":{}},{"cell_type":"markdown","source":"**Visualizing the Relationship Between Cholesterol Levels and Heart Disease by Smoking Status**\n\nTo explore the relationship between cholesterol levels and the likelihood of heart disease, while considering the influence of smoking status, I created a scatter plot with regression lines using Seaborn's lmplot. This visualization allows us to see how cholesterol levels correlate with heart disease across different smoking categories: Never, Former, and Current smokers.","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x=\"Cholesterol\", y=\"Heart Disease\", hue=\"Smoking\", data=heart_data)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:43.080131Z","iopub.execute_input":"2024-08-30T18:39:43.080652Z","iopub.status.idle":"2024-08-30T18:39:44.255171Z","shell.execute_reply.started":"2024-08-30T18:39:43.080612Z","shell.execute_reply":"2024-08-30T18:39:44.253859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation of the Plot**:\n\n* **Former Smokers**: The regression line for former smokers indicates a moderate positive correlation between cholesterol levels and heart disease risk in this group. As cholesterol levels increase, the likelihood of heart disease also increases, though the slope suggests a less steep rise compared to other groups.\n\n* **Never Smokers**: The line for never smokers shows a steeper positive correlation between cholesterol levels and heart disease. This indicates that in individuals who have never smoked, higher cholesterol levels are more strongly associated with an increased risk of heart disease.\n\n* **Current Smokers**: The regression line for current smokers suggests a significant, but slightly less pronounced, relationship between cholesterol levels and heart disease compared to never smokers.","metadata":{}},{"cell_type":"markdown","source":"**Distribution of Blood Pressure by Heart Disease Status**\n\nTo investigate how blood pressure levels are distributed among individuals with and without heart disease, I generated a histogram using Seaborn's histplot function. The histogram illustrates the distribution of blood pressure readings, with the bars colored to differentiate between individuals who have heart disease (1) and those who do not (0).","metadata":{}},{"cell_type":"code","source":"sns.histplot(data=heart_data, x='Blood Pressure', hue='Heart Disease')","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:44.256849Z","iopub.execute_input":"2024-08-30T18:39:44.257647Z","iopub.status.idle":"2024-08-30T18:39:44.640272Z","shell.execute_reply.started":"2024-08-30T18:39:44.257538Z","shell.execute_reply":"2024-08-30T18:39:44.639004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation of the Plot**:\n\nThe histogram shows a range of blood pressure levels across the dataset, with clear differences between the two groups:\n\n* **Individuals without Heart Disease (Hue 0)**: The distribution is relatively uniform across various blood pressure ranges, with a noticeable number of individuals having lower blood pressure levels (around 100-120 mmHg).\n\n* **Individuals with Heart Disease (Hue 1)**: The distribution indicates a higher frequency of heart disease cases at elevated blood pressure levels, particularly around 140-160 mmHg.\n\nThis plot suggests a potential relationship between higher blood pressure levels and the prevalence of heart disease, as indicated by the increased number of heart disease cases in the higher blood pressure ranges.","metadata":{}},{"cell_type":"markdown","source":"**Distribution of Smoking Status by Heart Disease**\n\nTo examine how smoking status correlates with the incidence of heart disease, I created a histogram using Seaborn's histplot function. The plot displays the count of individuals in each smoking category—Current, Never, and Former—divided by whether they have heart disease (1) or not (0).","metadata":{}},{"cell_type":"code","source":"sns.histplot(data=heart_data, x='Smoking', hue='Heart Disease')","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:44.642396Z","iopub.execute_input":"2024-08-30T18:39:44.643178Z","iopub.status.idle":"2024-08-30T18:39:44.954906Z","shell.execute_reply.started":"2024-08-30T18:39:44.643135Z","shell.execute_reply":"2024-08-30T18:39:44.953791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation of the Plot**:\n\nThe histogram visualizes the distribution of smoking status among individuals with and without heart disease:\n\n* **Current Smokers**: The distribution shows a fairly even split between those with and without heart disease, with a slight dominance of non-heart disease cases.\n\n* **Never Smokers**: This group also exhibits a relatively balanced distribution, with similar counts of individuals with and without heart disease.\n\n* **Former Smokers**: The plot shows a higher overall count in this category, with a noticeable prevalence of individuals without heart disease compared to those with heart disease.","metadata":{}},{"cell_type":"markdown","source":"**Distribution of Blood Sugar Levels by Heart Disease**\n\nTo explore the relationship between blood sugar levels and heart disease, I utilized Seaborn's histplot function to visualize the distribution of individuals based on their blood sugar levels, categorized by the presence of heart disease (1) or absence of it (0).","metadata":{}},{"cell_type":"code","source":"sns.histplot(data=heart_data, x='Blood Sugar', hue='Heart Disease')","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:44.95661Z","iopub.execute_input":"2024-08-30T18:39:44.957268Z","iopub.status.idle":"2024-08-30T18:39:45.344228Z","shell.execute_reply.started":"2024-08-30T18:39:44.957229Z","shell.execute_reply":"2024-08-30T18:39:45.342978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation of the Plot**:\n\nThe histogram presents the distribution of blood sugar levels among individuals with and without heart disease:\n\n* **Blood Sugar Levels (80 - 200 mg/dL)**: The plot shows a relatively even distribution of individuals across various blood sugar levels. In each blood sugar category, there is a noticeable presence of both heart disease and non-heart disease cases.\n\n* **High Blood Sugar Levels (160 - 200 mg/dL)**: In the higher blood sugar categories, there is a slightly larger count of individuals without heart disease compared to those with heart disease.","metadata":{}},{"cell_type":"markdown","source":"**Distribution of Stress Levels by Heart Disease**\n\nTo examine the relationship between stress levels and the presence of heart disease, I used Seaborn's histplot function to visualize the distribution of individuals according to their stress levels, with a distinction made between those with and without heart disease.","metadata":{}},{"cell_type":"code","source":"sns.histplot(data=heart_data, x='Stress Level', hue='Heart Disease')","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:45.345782Z","iopub.execute_input":"2024-08-30T18:39:45.346202Z","iopub.status.idle":"2024-08-30T18:39:45.729183Z","shell.execute_reply.started":"2024-08-30T18:39:45.346164Z","shell.execute_reply":"2024-08-30T18:39:45.7279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation of the Plot**:\n\nThe histogram reveals that individuals with stress levels between 6 and 10 tend to have a higher incidence of heart disease compared to those with stress levels between 1 and 5. Notably, however, there is an exception within the lower stress levels: people with stress levels of 2 and 3 also show a higher prevalence of heart disease, contrasting with the general trend observed in the lower stress range.","metadata":{}},{"cell_type":"markdown","source":"**Distribution of Age by Heart Disease**\n\nTo explore how age correlates with the presence of heart disease, I plotted a histogram using Seaborn’s histplot function, displaying the distribution of ages while distinguishing between individuals with and without heart disease.","metadata":{}},{"cell_type":"code","source":"sns.histplot(data=heart_data, x='Age', hue='Heart Disease')","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:45.733174Z","iopub.execute_input":"2024-08-30T18:39:45.733512Z","iopub.status.idle":"2024-08-30T18:39:46.11475Z","shell.execute_reply.started":"2024-08-30T18:39:45.733483Z","shell.execute_reply":"2024-08-30T18:39:46.113305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation of the Plot**:\n\nThe histogram reveals a clear age-related trend in the occurrence of heart disease. Individuals under the age of 50 predominantly do not have heart disease, as shown by the higher count in the \"no heart disease\" category. However, from the age of 50 onwards, the distribution shifts significantly, with a higher proportion of individuals suffering from heart disease. This trend becomes more pronounced as age increases, particularly in the 60 to 80 age range, where heart disease is more prevalent.","metadata":{}},{"cell_type":"markdown","source":"# **Feature Importance Analysis Using Mutual Information**\n\nTo better understand which features in the dataset are most relevant to predicting heart disease, I used Mutual Information (MI), a technique that measures the dependency between each feature and the target variable.","metadata":{}},{"cell_type":"code","source":"X = heart_data.copy()\ny = X.pop(\"Heart Disease\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\n\nfrom sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores[::3]  # show a few features with their MI scores","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:46.116392Z","iopub.execute_input":"2024-08-30T18:39:46.116887Z","iopub.status.idle":"2024-08-30T18:39:47.122146Z","shell.execute_reply.started":"2024-08-30T18:39:46.116846Z","shell.execute_reply":"2024-08-30T18:39:47.121094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation of MI Scores**:\n\nThe Mutual Information scores for a selection of features reveal the strength of their relationship with the presence of heart disease:\n\n* **Cholesterol**: With an MI score of 0.468090, cholesterol is the most significant feature related to heart disease among the selected features.\n* **Gender**: Although less impactful, gender still shows some dependency with an MI score of 0.016676.\n* **Smoking**: This feature has a lower MI score of 0.010393, indicating a weaker but still relevant relationship.\n* **Heart Rate**: The MI score for heart rate is quite low at 0.001186, suggesting minimal relevance.\n* **Diabetes**: Interestingly, diabetes has an MI score of 0.000000, indicating no measurable dependency with heart disease in this context.\n\nThis analysis helps to identify which features should be prioritized in predictive modeling efforts for heart disease, guiding the selection of variables for further investigation and model development.","metadata":{}},{"cell_type":"markdown","source":"# **Data Splitting and Feature Selection**\n\nTo prepare the data for machine learning, I split the dataset into training and validation subsets and selected relevant features for modeling.\n\n\n**Splitting the Data**:\nThe dataset is divided into training (80%) and validation (20%) subsets. The target variable y is 'Heart Disease,' while X consists of all other features. The random_state=0 parameter ensures that the results are reproducible.\n\n**Feature Selection**:\n* **Categorical Columns**: I selected categorical features with low cardinality (fewer than 10 unique values). These are typically more manageable for machine learning models, and the selection criterion ensures that only categorical columns are included.\n\n* **Numerical Columns**: I also selected all numerical features, as they generally provide valuable information for predicting the target variable.\n\n* **Combining Features**: The selected categorical and numerical columns are then combined to form the final training (X_train) and validation (X_valid) datasets.\n\nFinally, the code checks the data type of the target variable y, confirming that it is an integer (int64). This ensures that the target variable is correctly formatted for subsequent machine learning models.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Separate target from predictors\ny = heart_data['Heart Disease']\nX = heart_data.drop(['Heart Disease'], axis=1)\n\n# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\ny.dtype\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:47.123809Z","iopub.execute_input":"2024-08-30T18:39:47.12427Z","iopub.status.idle":"2024-08-30T18:39:47.146326Z","shell.execute_reply.started":"2024-08-30T18:39:47.124227Z","shell.execute_reply":"2024-08-30T18:39:47.145129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing and Model Building**\n\nTo build an effective machine learning model, I employed preprocessing steps tailored to both numerical and categorical data, followed by modeling using the XGBoost classifier.\n\n* **Numerical Data**: For numerical features, I used a SimpleImputer with a mean strategy to fill in any missing values. This ensures that the model doesn't encounter any null values, which could otherwise cause errors.\n\n* **Categorical Data**: For categorical features, I created a pipeline that first imputes missing values using the most frequent value (SimpleImputer) and then applies one-hot encoding (OneHotEncoder). This transformation converts categorical variables into a format suitable for the model by creating binary columns for each category, while ignoring any unseen categories in new data.\n\n* **Combining Transformations**: I combined the numerical and categorical preprocessing steps using ColumnTransformer, ensuring that the right transformations are applied to the correct types of data.","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:47.147724Z","iopub.execute_input":"2024-08-30T18:39:47.14816Z","iopub.status.idle":"2024-08-30T18:39:47.171491Z","shell.execute_reply.started":"2024-08-30T18:39:47.148122Z","shell.execute_reply":"2024-08-30T18:39:47.169886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building and Evaluation\n\n* **Model Choice**: I selected the XGBClassifier, a robust model well-suited for classification tasks, and fine-tuned it with 500 estimators and a learning rate of 0.05. These parameters aim to balance model performance and training time.\n\n* **Pipeline Creation**: I combined the preprocessing steps and the model into a single pipeline. This approach ensures that the same transformations are applied consistently to both the training and validation data, reducing the risk of data leakage.\n\n* **Model Training and Prediction**: The pipeline was fitted on the training data, and predictions were made on the validation set.\n\n* **Model Evaluation**: I evaluated the model's performance using accuracy, confusion matrix, and classification report.\n\nThis evaluation suggests that while the model has room for improvement, it demonstrates a reasonable balance between precision and recall, particularly given the complexity of predicting heart disease.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier(n_estimators=500, learning_rate=0.05, random_state=0)\n\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\naccuracy = accuracy_score(y_valid, preds)\nprint('Accuracy:', accuracy)\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_valid, preds))\nprint('Classification Report:')\nprint(classification_report(y_valid, preds))","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:39:47.172833Z","iopub.execute_input":"2024-08-30T18:39:47.173181Z","iopub.status.idle":"2024-08-30T18:39:47.823927Z","shell.execute_reply.started":"2024-08-30T18:39:47.173151Z","shell.execute_reply":"2024-08-30T18:39:47.822799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation**:\n\n* **Accuracy**: The model achieved an accuracy of 60.5%, indicating that it correctly predicted the presence or absence of heart disease 60.5% of the time.\n\n* **Confusion Matrix**: The confusion matrix shows that out of 200 cases, 72 were true negatives (correctly identified as not having heart disease), and 49 were true positives (correctly identified as having heart disease). However, there were 40 false positives and 39 false negatives.\n\n* **Classification Report**: The classification report provides precision, recall, and F1-score for both classes. The model performs slightly better in predicting the absence of heart disease (class 0) than its presence (class 1).\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Logistic Regression Model for Heart Disease Prediction**\n\nIn addition to the XGBoost classifier, I also implemented a Logistic Regression model to predict heart disease. Logistic Regression is a linear model that is widely used for binary classification tasks.\n\n\n# Data Preprocessing and Model Setup\n\n**Standardization**: Since Logistic Regression benefits from standardized features, I included standard scaling in the preprocessing pipeline to normalize numerical features. However, this step is implicitly handled by the pipeline.\n\n**Logistic Regression Model**: The Logistic Regression model was configured with the following parameters:\n\n* **C=0.5**: This parameter controls the regularization strength. A smaller value of C increases regularization, helping prevent overfitting.\n* **penalty='l2'**: L2 regularization was applied to penalize large coefficients, thus stabilizing the model and improving generalization.\n* **solver='lbfgs'**: The lbfgs solver is an efficient algorithm for logistic regression, especially with a large number of features.\n* **max_iter=200**: The maximum number of iterations was set to 200 to ensure convergence.\n* **class_weight='balanced'**: This option was used to address class imbalance by adjusting the weights of the classes inversely proportional to their frequencies.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(\n    C=0.5,                  # Example regularization strength\n    penalty='l2',            # L2 regularization\n    solver='lbfgs',         # Solver algorithm\n    max_iter=200,           # Maximum iterations\n    class_weight='balanced' # Handle class imbalance\n)\n\n\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\naccuracy = accuracy_score(y_valid, preds)\nprint('Accuracy:', accuracy)\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_valid, preds))\nprint('Classification Report:')\nprint(classification_report(y_valid, preds))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T18:54:39.136167Z","iopub.execute_input":"2024-08-30T18:54:39.13661Z","iopub.status.idle":"2024-08-30T18:54:39.252552Z","shell.execute_reply.started":"2024-08-30T18:54:39.136557Z","shell.execute_reply":"2024-08-30T18:54:39.251355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building and Evaluation\n\n* **Pipeline Integration**: The model was integrated into a pipeline along with the preprocessing steps. This ensures consistent application of preprocessing to both training and validation datasets.\n\n* **Model Training and Predictions**: The pipeline was fitted to the training data, and predictions were generated for the validation set.\n\n* **Model Evaluation**: The model was evaluated using accuracy, confusion matrix, and classification report.\n\n* **Accuracy**: The Logistic Regression model achieved an accuracy of 58%, meaning it correctly predicted the presence or absence of heart disease 58% of the time.\n\n* **Confusion Matrix**: The confusion matrix shows that out of 200 cases, 67 were true negatives (correctly identified as not having heart disease) and 49 were true positives (correctly identified as having heart disease). However, there were 45 false positives and 39 false negatives.\n\n# Classification Report:\n\n* **Class 0 (No Heart Disease)**: The precision, recall, and F1-score for class 0 were 0.63, 0.60, and 0.61, respectively. This indicates that the model performs slightly better in predicting the absence of heart disease.\n* **Class 1 (Heart Disease)**: The precision, recall, and F1-score for class 1 were lower, at 0.52, 0.56, and 0.54, respectively, indicating some challenges in predicting heart disease cases.\n* **Macro Average and Weighted Average**: Both averages hover around 0.58, reflecting the model's balanced performance across both classes.\n\n# Insights\nThe Logistic Regression model shows moderate performance with an accuracy of 58%, slightly lower than the XGBoost model. While the model does well in handling class imbalance, as evidenced by balanced precision and recall, it still struggles to differentiate between heart disease and non-heart disease cases, particularly with precision for predicting heart disease (class 1).\n\nThis model could potentially be improved with additional feature engineering, hyperparameter tuning, or by exploring more complex models. Nonetheless, it provides a baseline for linear classification in this context.","metadata":{}},{"cell_type":"markdown","source":"# **Conclusion on Model Performance for Heart Disease Prediction**\nIn this notebook, we explored two different machine learning models—XGBoost and Logistic Regression—to predict heart disease based on a variety of patient features. The models were developed and evaluated with preprocessing steps included in a pipeline to ensure consistent data handling.\n\n# 1. Exploratory Data Analysis (EDA) Insights\nBefore delving into model building, we conducted an EDA to understand the underlying patterns and relationships within the dataset. Some key observations included:\n\n**Cholesterol Levels**: Cholesterol was identified as a significant predictor of heart disease, exhibiting a relatively high mutual information (MI) score.\nGender and Smoking: These categorical features, although less influential than cholesterol, also showed a correlation with heart disease presence.\nOther Features: Factors like heart rate and diabetes showed minimal predictive power, as indicated by their low MI scores.\nThese insights guided us in selecting features and preprocessing strategies for the models.\n\n# 2. Model 1: XGBoost Classifier\n**Performance**: The XGBoost model achieved an accuracy of 60.5%. It demonstrated better performance than Logistic Regression, particularly in classifying the absence of heart disease (class 0).\n\n**Evaluation Metrics**:\n* **Confusion Matrix**: The model correctly identified 72 out of 112 negative cases (no heart disease) and 49 out of 88 positive cases (heart disease).\n* **Classification Report**: The F1-scores were 0.65 for class 0 and 0.55 for class 1, showing a stronger ability to identify non-heart disease cases but with some struggles in detecting heart disease cases.\n\n# 3. Model 2: Logistic Regression\nPerformance: The Logistic Regression model showed a slightly lower accuracy of 58%. It performed similarly to XGBoost in classifying class 0 but was less effective overall.\n\n**Evaluation Metrics**:\n* **Confusion Matrix**: This model correctly predicted 67 out of 112 negative cases and 49 out of 88 positive cases, with a similar number of false positives and false negatives as XGBoost.\n* **Classification Report**: The F1-scores were 0.61 for class 0 and 0.54 for class 1, indicating a modest performance in predicting heart disease.\n\n# 4. Comparative Insights\n* **Accuracy**: Both models performed relatively close to each other, with XGBoost having a slight edge in accuracy.\n* **Precision and Recall**: XGBoost exhibited a balanced trade-off between precision and recall, especially for the more challenging task of predicting heart disease (class 1). Logistic Regression, while simpler, did not match the performance of XGBoost.\n* **Handling Imbalance**: Both models were designed to handle class imbalance, with Logistic Regression using the class_weight='balanced' parameter and XGBoost utilizing its inherent capability to deal with imbalance.\n\n# 5. Final Thoughts\nWhile the XGBoost model outperformed Logistic Regression in this notebook, both models struggled to achieve high precision and recall for the heart disease class (class 1). This suggests that further improvements could be made, possibly through more sophisticated feature engineering, tuning of model hyperparameters, or exploring other advanced algorithms.\n\n\n# **In summary**:\n\n**XGBoost**: Better overall accuracy and balance between precision and recall.\n**Logistic Regression**: A simpler model with decent performance, but less effective compared to XGBoost.\n\nGiven the medical context of this problem, where the cost of false negatives (failing to predict heart disease) could be significant, additional focus on improving recall for the heart disease class would be crucial in a real-world application. Future work might also include cross-validation and exploration of ensemble techniques to boost model performance.","metadata":{}}]}