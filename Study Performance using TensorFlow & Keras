{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9189698,"sourceType":"datasetVersion","datasetId":5555170}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nukimayasari/student-performance-using-tensorflow-keras?scriptVersionId=197812484\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-22T17:19:08.72879Z","iopub.execute_input":"2024-09-22T17:19:08.729342Z","iopub.status.idle":"2024-09-22T17:19:08.738166Z","shell.execute_reply.started":"2024-09-22T17:19:08.729289Z","shell.execute_reply":"2024-09-22T17:19:08.737086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Model for Predicting Final Grade\n\nThis notebook demonstrates the process of building a deep learning model to predict `FinalGrade` based on 6 features. Although the dataset is small (10 rows and 8 columns), this project serves as a learning experience in constructing and evaluating neural networks using `TensorFlow` and `Keras`.\n","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/student-performance-predictions/student_performance.csv', index_col=0)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T17:19:08.740115Z","iopub.execute_input":"2024-09-22T17:19:08.740933Z","iopub.status.idle":"2024-09-22T17:19:08.771332Z","shell.execute_reply.started":"2024-09-22T17:19:08.740894Z","shell.execute_reply":"2024-09-22T17:19:08.770035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This dataset consists of 10 student records with 8 columns: `Name`, `Gender`, `AttendanceRate`, `StudyHoursPerWeek`, `PreviousGrade`, `ExtracurricularActivities`, `ParentalSupport`, and `FinalGrade`. Descriptive statistics provide insights into key numerical variables:\n\n- **AttendanceRate**: On average, students have an attendance rate of 85.6%, ranging from 70% to 95%, with a standard deviation of 7.44%.\n- **StudyHoursPerWeek**: Students study for an average of 17.7 hours per week, with a minimum of 8 and a maximum of 30 hours. The variability in study time is notable (std of 6.85).\n- **PreviousGrade**: The average previous grade is 78.1, with grades ranging from 60 to 90. The distribution shows moderate spread (std of 10.17).\n- **ExtracurricularActivities**: The median student participates in 1 to 2 extracurricular activities, with a range from 0 to 3.\n- **FinalGrade**: Students have an average final grade of 80.2, with scores varying between 62 and 92.\n\nThese statistics highlight varying attendance, study habits, and performance levels among the students, offering a solid foundation for further analysis and modeling.","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T17:19:08.772841Z","iopub.execute_input":"2024-09-22T17:19:08.773766Z","iopub.status.idle":"2024-09-22T17:19:08.80321Z","shell.execute_reply.started":"2024-09-22T17:19:08.773698Z","shell.execute_reply":"2024-09-22T17:19:08.801971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-22T17:19:08.805859Z","iopub.execute_input":"2024-09-22T17:19:08.806897Z","iopub.status.idle":"2024-09-22T17:19:08.814652Z","shell.execute_reply.started":"2024-09-22T17:19:08.806842Z","shell.execute_reply":"2024-09-22T17:19:08.813504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns.tolist","metadata":{"execution":{"iopub.status.busy":"2024-09-22T17:19:08.816264Z","iopub.execute_input":"2024-09-22T17:19:08.817634Z","iopub.status.idle":"2024-09-22T17:19:08.826962Z","shell.execute_reply.started":"2024-09-22T17:19:08.817584Z","shell.execute_reply":"2024-09-22T17:19:08.825831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this section, we prepare the dataset for training a deep learning model using **TensorFlow**. The dataset consists of both numerical and categorical features. The target variable, `FinalGrade`, is used to predict student performance.\n\n\n### Steps:\n1. **Feature Selection**: We select six features (`Gender`, `AttendanceRate`, `StudyHoursPerWeek`, `PreviousGrade`, `ExtracurricularActivities`, and `ParentalSupport`) as input for the model. These features are divided into:\n   - **Numerical Features**: `AttendanceRate`, `StudyHoursPerWeek`, `PreviousGrade`, `ExtracurricularActivities`\n   - **Categorical Features**: `Gender`, `ParentalSupport`\n\n\n2. **Preprocessing**:\n   - **Numerical Data**: Standardized using `StandardScaler`, which scales the data to have a mean of 0 and a standard deviation of 1, ensuring that all numeric features are on a similar scale.\n   - **Categorical Data**: Encoded using `OneHotEncoder`, converting categorical features into binary (0, 1) format.\n\n\n3. **Train-Test Split**: The data is split into training and validation sets using `train_test_split`, with 70% of the data for training and 30% for validation. This ensures that the model can be tested on unseen data for generalization.\n\n\n4. **Data Transformation**: The preprocessor is applied to transform both the training and validation datasets. The target variable, `FinalGrade`, is scaled down by dividing it by 100 to normalize the range.\n\n\n5. **Input Shape for Model**: After preprocessing, the input data has 9 features (due to one-hot encoding of categorical variables), setting the input shape for the deep learning model as `[9]`.\n\n\nThis preprocessing pipeline sets the foundation for building and training a neural network model.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import GroupShuffleSplit, train_test_split\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\nfeatures=['Gender', 'AttendanceRate', 'StudyHoursPerWeek', 'PreviousGrade', 'ExtracurricularActivities', 'ParentalSupport']\nX = data[features]\ny = data.FinalGrade\n\nfeatures_num = ['AttendanceRate', 'StudyHoursPerWeek', 'PreviousGrade', 'ExtracurricularActivities']\nfeatures_cat = ['Gender', 'ParentalSupport']\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), features_num),\n    (OneHotEncoder(), features_cat),\n)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.7, test_size=0.3,\n                                                                random_state=0)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\ny_train = y_train / 100\ny_valid = y_valid / 100\n\ninput_shape = [X_train.shape[1]] \nprint(\"Input shape: {}\".format(input_shape))","metadata":{"execution":{"iopub.status.busy":"2024-09-22T17:19:08.828571Z","iopub.execute_input":"2024-09-22T17:19:08.829022Z","iopub.status.idle":"2024-09-22T17:19:08.856877Z","shell.execute_reply.started":"2024-09-22T17:19:08.828976Z","shell.execute_reply":"2024-09-22T17:19:08.855553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training and Performance\n\nTo predict the final grades, I designed a deep learning model using **Keras**. The model consists of multiple layers, including **Batch Normalization** to stabilize and speed up training, and **Dense layers** with 512 neurons each, activated by **ReLU**. This structure allows the model to capture complex, non-linear relationships in the data. The model’s output layer consists of a single neuron since we are performing a regression task to predict final grades.\n\nI used the **Adam optimizer** for its adaptive nature, along with **Mean Absolute Error (MAE)** as the loss function, which is well-suited for regression tasks. The model was compiled to minimize this loss over the course of training.\n\n### Callbacks and Optimizations\nTo prevent overfitting and ensure better generalization to unseen data, I incorporated two important callbacks:\n\n- **EarlyStopping**: This callback monitors the validation loss and stops the training process if the model doesn’t improve after 15 epochs. It also restores the model's weights from the epoch with the lowest validation loss.\n- **ReduceLROnPlateau**: This callback reduces the learning rate by half when the validation loss plateaus, helping the model converge more effectively without overshooting.\n\n### Training Results\n\nAfter training the model for 60 epochs, the results are plotted in the graph below. The **training loss** (blue line) shows a sharp initial decrease, meaning the model learned rapidly in the early epochs. Afterward, it continued to decrease steadily. The **validation loss** (orange line), which measures performance on unseen data, followed a similar trajectory, indicating that the model is not overfitting and generalizing well.\n\nThe **minimum validation loss** reached was **0.057**, meaning the model’s average error is around 5.98% when predicting the final grades. This is a strong indicator that the model is performing well, with a low error on both the training and validation sets.","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n    \nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n    metrics=['mae'],\n)\n\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=15,  # Increase patience to allow more time for improvement\n    restore_best_weights=True\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,  # Reduce the learning rate by half instead of 0.1\n    patience=5,\n    min_lr=1e-6  # Set a smaller minimum learning rate\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=64,  # You can experiment with smaller/larger batch sizes too\n    epochs=100,     # Keep more epochs to see the effect of the changes\n    verbose=0,\n    callbacks=[early_stopping, lr_scheduler]\n)\n\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[0:, ['loss', 'val_loss']].plot()\nprint((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))","metadata":{"execution":{"iopub.status.busy":"2024-09-22T17:19:08.858521Z","iopub.execute_input":"2024-09-22T17:19:08.858981Z","iopub.status.idle":"2024-09-22T17:19:15.577372Z","shell.execute_reply.started":"2024-09-22T17:19:08.858932Z","shell.execute_reply":"2024-09-22T17:19:15.5759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion and Takeaways\n\nIn this project, we successfully built and trained a deep learning model to predict final grades based on a variety of student performance factors. The model, incorporating batch normalization and callbacks like early stopping and learning rate reduction, achieved a low validation loss of **0.0598**, indicating strong predictive performance.\n\n### Key Takeaways:\n- **Deep Learning**: A multi-layer neural network with batch normalization effectively captured the relationships between student data and final grades.\n- **Model Generalization**: The use of early stopping and learning rate scheduling helped prevent overfitting, ensuring that the model generalizes well to unseen data.\n- **Feature Scaling**: Proper preprocessing of numerical and categorical features was essential for improving the model's convergence and performance.\n\nThese insights demonstrate the potential of deep learning in educational data analysis, showing how it can provide accurate predictions for student outcomes. Given the small dataset, the model performed well. Though results could improve with a larger, more diverse dataset","metadata":{}}]}